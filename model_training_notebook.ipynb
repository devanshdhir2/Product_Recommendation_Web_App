{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13421113,"sourceType":"datasetVersion","datasetId":8518177}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-18T08:30:05.334740Z","iopub.execute_input":"2025-10-18T08:30:05.335319Z","iopub.status.idle":"2025-10-18T08:30:05.714461Z","shell.execute_reply.started":"2025-10-18T08:30:05.335299Z","shell.execute_reply":"2025-10-18T08:30:05.713769Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cleaned-intern-data/cleaned_intern_data.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi_key= user_secrets.get_secret(\"PINECONE_API_KEY\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T08:30:05.715865Z","iopub.execute_input":"2025-10-18T08:30:05.716297Z","iopub.status.idle":"2025-10-18T08:30:05.797821Z","shell.execute_reply.started":"2025-10-18T08:30:05.716277Z","shell.execute_reply":"2025-10-18T08:30:05.797151Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"**creating a single, unified embedding for each product that represents both its textual description and its visual appearance.**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Load the cleaned dataset\ndf = pd.read_csv('/kaggle/input/cleaned-intern-data/cleaned_intern_data.csv')\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T08:30:05.798550Z","iopub.execute_input":"2025-10-18T08:30:05.798796Z","iopub.status.idle":"2025-10-18T08:30:05.821200Z","shell.execute_reply.started":"2025-10-18T08:30:05.798770Z","shell.execute_reply":"2025-10-18T08:30:05.820338Z"}},"outputs":[{"name":"stdout","text":"                                               title            brand  \\\n0  GOYMFK 1pc Free Standing Shoe Rack, Multi-laye...           GOYMFK   \n1  Plant Repotting Mat MUYETOL Waterproof Transpl...          MUYETOL   \n2  Pickleball Doormat, Welcome Doormat Absorbent ...          VEWETOL   \n3  JOIN IRON Foldable TV Trays for Eating Set of ...  JOIN IRON Store   \n4  Folews Bathroom Organizer Over The Toilet Stor...     Folews Store   \n\n                                         description  price  \\\n0  multiple shoes, coats, hats, and other items E...  24.99   \n1  Plant Repotting Mat MUYETOL Waterproof Transpl...   5.98   \n2  The decorative doormat features a subtle textu...  13.99   \n3  Set of Four Folding Trays With Matching Storag...  89.99   \n4  Folews Bathroom Organizer Over The Toilet Stor...  63.99   \n\n                                          categories  \\\n0  ['Home & Kitchen', 'Storage & Organization', '...   \n1  ['Patio, Lawn & Garden', 'Outdoor Décor', 'Doo...   \n2  ['Patio, Lawn & Garden', 'Outdoor Décor', 'Doo...   \n3  ['Home & Kitchen', 'Furniture', 'Game & Recrea...   \n4  ['Home & Kitchen', 'Furniture', 'Bathroom Furn...   \n\n                                              images manufacturer  \\\n0  ['https://m.media-amazon.com/images/I/416WaLx1...       GOYMFK   \n1  ['https://m.media-amazon.com/images/I/41RgefVq...      MUYETOL   \n2  ['https://m.media-amazon.com/images/I/61vz1Igl...    Contrence   \n3  ['https://m.media-amazon.com/images/I/41p4d4VJ...          NaN   \n4  ['https://m.media-amazon.com/images/I/41ixgM73...       Folews   \n\n         package_dimensions country_of_origin      material          color  \\\n0  2.36\"D x 7.87\"W x 21.6\"H             China         Metal          White   \n1           26.8\"L x 26.8\"W               NaN  Polyethylene          Green   \n2               24\"L x 16\"W               NaN        Rubber          A5589   \n3    18.9\"D x 14.2\"W x 26\"H               NaN          Iron  Grey Set of 4   \n4  12.6\"D x 25.2\"W x 68.5\"H             China           NaN            NaN   \n\n                                uniq_id  \n0  02593e81-5c09-5069-8516-b0b29f439ded  \n1  b2ede786-3f51-5a45-9a5b-bcf856958cd8  \n2  8fd9377b-cfa6-5f10-835c-6b8eca2816b5  \n3  bdc9aa30-9439-50dc-8e89-213ea211d66a  \n4  aba4138e-6401-52ca-a099-02e30b638db4  \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"#!pip install sentence-transformers\nfrom sentence_transformers import SentenceTransformer\n# Load the pre-trained model\ntext_embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n# Create a combined text field for embedding. This is our key feature engineering step.\ndf['combined_text'] = (\n    df['title'].fillna('') + '. ' +\n    df['description'].fillna('') + '. ' +\n    df['categories'].fillna('') + '. Material: ' +\n    df['material'].fillna('') + '. Color: ' +\n    df['color'].fillna('')\n)\n# Generate embeddings for the combined text\n# This may take a few minutes depending on your hardware\ntext_embeddings = text_embedding_model.encode(df['combined_text'].tolist(), show_progress_bar=True)\nprint(f\"Generated text embeddings with shape: {text_embeddings.shape}\")\n# Expected output shape: (210, 384)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T08:30:05.822779Z","iopub.execute_input":"2025-10-18T08:30:05.823059Z","iopub.status.idle":"2025-10-18T08:30:21.001227Z","shell.execute_reply.started":"2025-10-18T08:30:05.823040Z","shell.execute_reply":"2025-10-18T08:30:21.000579Z"}},"outputs":[{"name":"stderr","text":"2025-10-18 08:30:12.088687: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760776212.112858     120 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760776212.119852     120 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a907f5dbb770471f9ea59d83ab97740c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c29c7d42bb24e94a6b18468cdc5d534"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b696f6d877c84f2cbbf8af885a3b2299"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc72b608e17e4363b1c15b3f952f8507"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9a81a28f50b4996be4d4c3c3699185b"}},"metadata":{}},{"name":"stdout","text":"Generated text embeddings with shape: (210, 384)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport numpy as np\nimport ast\nfrom tqdm import tqdm\nimport pandas as pd\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the pre-trained ResNet50 and remove final FC layer\nresnet50 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\nresnet50.eval()\n# keep everything except the final fully-connected layer\nimage_embedding_model = torch.nn.Sequential(*list(resnet50.children())[:-1])\nimage_embedding_model.to(device)\nimage_embedding_model.eval()\n\n# Preprocessing (ImageNet)\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225]),\n])\n\ndef get_image_embedding(image_url):\n    \"\"\"Download, preprocess and return a 1D numpy embedding of length 2048.\n       Returns np.zeros(2048) on failure.\"\"\"\n    if not isinstance(image_url, str) or not image_url:\n        return np.zeros(2048, dtype=np.float32)\n    try:\n        response = requests.get(image_url.strip(), timeout=10)\n        response.raise_for_status()\n        img = Image.open(BytesIO(response.content)).convert('RGB')\n        img_t = preprocess(img).unsqueeze(0).to(device)  # shape (1,3,224,224)\n        with torch.no_grad():\n            emb = image_embedding_model(img_t)           # shape (1,2048,1,1)\n            emb = emb.view(emb.size(0), -1)             # shape (1,2048)\n            emb = emb.cpu().numpy().squeeze(0)         # shape (2048,)\n        return emb.astype(np.float32)\n    except Exception as e:\n        # you can replace print with logging if you prefer\n        print(f\"Could not process image {image_url!r}: {e}\")\n        return np.zeros(2048, dtype=np.float32)\n\ndef get_first_image_url(row):\n    \"\"\"Return first URL from row['images'].\n       Supports: actual list, string representation of list, single URL string, NaN.\"\"\"\n    val = row.get('images') if isinstance(row, dict) else row['images']\n    # handle NaN / None\n    if val is None or (isinstance(val, float) and np.isnan(val)):\n        return None\n    # If it's already a list, return first element\n    if isinstance(val, list):\n        return val[0].strip() if val else None\n    # If it's a string, try to parse or use it directly\n    if isinstance(val, str):\n        s = val.strip()\n        # If string looks like a python list e.g. \"['url1', 'url2']\"\n        if s.startswith('[') and s.endswith(']'):\n            try:\n                parsed = ast.literal_eval(s)\n                if isinstance(parsed, list) and parsed:\n                    return parsed[0].strip()\n            except (ValueError, SyntaxError):\n                pass\n        # otherwise assume it's a single URL\n        return s\n    # else\n    return None\n\n# Example: assume df is already a pandas DataFrame with 'images' column\n# df['first_image_url'] = df.apply(get_first_image_url, axis=1)  # axis=1 if row-by-row\n# If df is large, vectorized approach (faster) — but apply is fine for moderate size.\n\ndf['first_image_url'] = df.apply(get_first_image_url, axis=1)\n\n# Generate embeddings (with progress bar). This will create shape (N, 2048)\nurls = df['first_image_url'].tolist()\nimage_embeddings = np.zeros((len(urls), 2048), dtype=np.float32)\nfor i, url in enumerate(tqdm(urls, desc=\"Embedding images\")):\n    image_embeddings[i] = get_image_embedding(url)\n\nprint(f\"Generated image embeddings with shape: {image_embeddings.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T08:30:25.833940Z","iopub.execute_input":"2025-10-18T08:30:25.834215Z","iopub.status.idle":"2025-10-18T08:30:53.129625Z","shell.execute_reply.started":"2025-10-18T08:30:25.834192Z","shell.execute_reply":"2025-10-18T08:30:53.128784Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 198MB/s] \nEmbedding images: 100%|██████████| 210/210 [00:26<00:00,  8.04it/s]","output_type":"stream"},{"name":"stdout","text":"Generated image embeddings with shape: (210, 2048)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Ensure both arrays have the same number of rows (products)\nassert text_embeddings.shape[0] == image_embeddings.shape[0], \\\n    f\"Row mismatch: text={text_embeddings.shape[0]} vs image={image_embeddings.shape[0]}\"\n\n# Concatenate along feature dimension\nmulti_modal_embeddings = np.concatenate([text_embeddings, image_embeddings], axis=1)\n\nprint(f\"Generated multi-modal embeddings with shape: {multi_modal_embeddings.shape}\")\n# Expected output: (210, 2432)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T08:30:59.950439Z","iopub.execute_input":"2025-10-18T08:30:59.951085Z","iopub.status.idle":"2025-10-18T08:30:59.955943Z","shell.execute_reply.started":"2025-10-18T08:30:59.951061Z","shell.execute_reply":"2025-10-18T08:30:59.955288Z"}},"outputs":[{"name":"stdout","text":"Generated multi-modal embeddings with shape: (210, 2432)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Install the official Pinecone package\n!pip uninstall -y pinecone-client\n!pip install -U pinecone\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T08:31:27.107691Z","iopub.execute_input":"2025-10-18T08:31:27.108478Z","iopub.status.idle":"2025-10-18T08:31:33.385813Z","shell.execute_reply.started":"2025-10-18T08:31:27.108444Z","shell.execute_reply":"2025-10-18T08:31:33.384994Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[33mWARNING: Skipping pinecone-client as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting pinecone\n  Downloading pinecone-7.3.0-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2025.8.3)\nCollecting pinecone-plugin-assistant<2.0.0,>=1.6.0 (from pinecone)\n  Downloading pinecone_plugin_assistant-1.8.0-py3-none-any.whl.metadata (30 kB)\nCollecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone)\n  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.9.0.post0)\nRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone) (4.15.0)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.5.0)\nCollecting packaging<25.0,>=24.2 (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone)\n  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.32.5)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.10)\nDownloading pinecone-7.3.0-py3-none-any.whl (587 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pinecone_plugin_assistant-1.8.0-py3-none-any.whl (259 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.3/259.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\nDownloading packaging-24.2-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pinecone-plugin-interface, packaging, pinecone-plugin-assistant, pinecone\n  Attempting uninstall: packaging\n    Found existing installation: packaging 25.0\n    Uninstalling packaging-25.0:\n      Successfully uninstalled packaging-25.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ndataproc-spark-connect 0.8.3 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed packaging-24.2 pinecone-7.3.0 pinecone-plugin-assistant-1.8.0 pinecone-plugin-interface-0.0.7\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Install the latest Pinecone SDK (not pinecone-client)\n# !pip -q install -U pinecone tqdm\n\nfrom pinecone import Pinecone, ServerlessSpec\nfrom kaggle_secrets import UserSecretsClient\nfrom tqdm import tqdm\nimport numpy as np\nimport math\n\n# 1) Init client (Kaggle secret)\napi_key = UserSecretsClient().get_secret(\"PINECONE_API_KEY\")\npc = Pinecone(api_key=api_key)\n\n# 2) Create a serverless index if needed\nindex_name = \"product-recommendations\"\nembedding_dim = int(multi_modal_embeddings.shape[1])  # 2432\n\nif index_name not in pc.list_indexes().names():\n    print(f\"Creating new serverless index '{index_name}'...\")\n    pc.create_index(\n        name=index_name,\n        dimension=embedding_dim,\n        metric=\"cosine\",\n        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n    )\n    print(\"Index created.\")\nelse:\n    print(f\"Index '{index_name}' already exists.\")\n\n# (optional) wait until ready\ndesc = pc.describe_index(index_name)\nindex = pc.Index(host=desc.host)\n\n# 3) Prepare data for upserting (dict format)\ndef _clean_meta(d):\n    out = {}\n    for k, v in d.items():\n        if v is None:\n            continue\n        if isinstance(v, float) and np.isnan(v):\n            continue\n        out[k] = v\n    return out\n\nvectors_to_upsert = []\n# reset_index ensures 'i' aligns with multi_modal_embeddings row i\nfor i, row in df.reset_index(drop=True).iterrows():\n    vector_id = str(row.get(\"uniq_id\", i))\n    values = multi_modal_embeddings[i].astype(float).tolist()\n    metadata = _clean_meta({\n        \"title\": row.get(\"title\"),\n        \"brand\": row.get(\"brand\"),\n        \"price\": row.get(\"price\"),\n        \"image_url\": row.get(\"first_image_url\"),\n    })\n    vectors_to_upsert.append({\"id\": vector_id, \"values\": values, \"metadata\": metadata})\n\n# 4) Upsert in batches\nbatch_size = 100\nprint(\"Upserting vectors to Pinecone...\")\nfor start in tqdm(range(0, len(vectors_to_upsert), batch_size)):\n    batch = vectors_to_upsert[start:start + batch_size]\n    index.upsert(vectors=batch)\n\n# 5) Check stats\nprint(\"\\nIndex stats:\")\nprint(index.describe_index_stats())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T08:32:50.665168Z","iopub.execute_input":"2025-10-18T08:32:50.665480Z","iopub.status.idle":"2025-10-18T08:33:17.538786Z","shell.execute_reply.started":"2025-10-18T08:32:50.665458Z","shell.execute_reply":"2025-10-18T08:33:17.538135Z"}},"outputs":[{"name":"stdout","text":"Creating new serverless index 'product-recommendations'...\nIndex created.\nUpserting vectors to Pinecone...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3/3 [00:02<00:00,  1.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nIndex stats:\n{'dimension': 2432,\n 'index_fullness': 0.0,\n 'metric': 'cosine',\n 'namespaces': {},\n 'total_vector_count': 0,\n 'vector_type': 'dense'}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import time\n\n# Wait for 30 seconds to allow the index to update\nprint(\"Waiting for 30 seconds for the index to update...\")\ntime.sleep(30)\n\n# Check the stats again\nprint(\"\\nUpdated Index stats:\")\nprint(index.describe_index_stats())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T08:33:25.057589Z","iopub.execute_input":"2025-10-18T08:33:25.058126Z","iopub.status.idle":"2025-10-18T08:33:55.103940Z","shell.execute_reply.started":"2025-10-18T08:33:25.058101Z","shell.execute_reply":"2025-10-18T08:33:55.103261Z"}},"outputs":[{"name":"stdout","text":"Waiting for 30 seconds for the index to update...\n\nUpdated Index stats:\n{'dimension': 2432,\n 'index_fullness': 0.0,\n 'metric': 'cosine',\n 'namespaces': {'': {'vector_count': 210}},\n 'total_vector_count': 210,\n 'vector_type': 'dense'}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# PREREQS (run once)\n!pip -q install -U \"pinecone==5.*\" \"transformers==4.46.2\" \"accelerate>=0.34.2\" \"sentence-transformers>=3.1.1\" \"tqdm\"\n\n#  CONNECT TO PINECONE\nfrom kaggle_secrets import UserSecretsClient\nfrom pinecone import Pinecone\nimport pandas as pd\n\nuser_secrets = UserSecretsClient()\nPINECONE_API_KEY = user_secrets.get_secret(\"PINECONE_API_KEY\")\n\npc = Pinecone(api_key=PINECONE_API_KEY)\nINDEX_NAME = \"product-recommendations\"\n\n# Use data-plane host for speed\nidx_desc = pc.describe_index(INDEX_NAME)\nindex = pc.Index(host=idx_desc.host)\n\nstats = index.describe_index_stats()\nprint(\"Index connected. Stats:\", stats)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T09:49:04.762899Z","iopub.execute_input":"2025-10-18T09:49:04.763396Z","iopub.status.idle":"2025-10-18T09:50:34.839498Z","shell.execute_reply.started":"2025-10-18T09:49:04.763372Z","shell.execute_reply":"2025-10-18T09:50:34.838809Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.3/427.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.9/374.9 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.6/486.6 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mIndex connected. Stats: {'dimension': 2432,\n 'index_fullness': 0.0,\n 'namespaces': {'': {'vector_count': 210}},\n 'total_vector_count': 210}\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# RETRIEVAL HELPERS\n\nfrom sentence_transformers import SentenceTransformer\n\nTEXT_DIM, IMG_DIM = 384, 2048\nenc = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n\ndef encode_query_mm(q: str, w_text: float = 1.0):\n    v = enc.encode(q, normalize_embeddings=True).tolist()\n    v = [w_text * x for x in v]\n    return v + [0.0] * IMG_DIM\n\n\ndef search(query: str, top_k: int = 5, w_text: float = 1.0, filt: dict | None = None) -> pd.DataFrame:\n    qvec = encode_query_mm(query, w_text=w_text)\n    res = index.query(vector=qvec, top_k=top_k, include_metadata=True, filter=filt or {})\n    rows = []\n    for m in res.get(\"matches\", []):\n        md = m.get(\"metadata\", {})\n        rows.append({\n            \"id\": m.get(\"id\"),\n            \"score\": float(m.get(\"score\", 0.0)),\n            \"title\": md.get(\"title\"),\n            \"brand\": md.get(\"brand\"),\n            \"price\": md.get(\"price\"),\n            \"image_url\": md.get(\"image_url\"),\n        })\n    return pd.DataFrame(rows)\n\n# Smoke test\nhits = search(\"sofa\", top_k=5)\nprint(hits.head(3))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T09:50:42.761467Z","iopub.execute_input":"2025-10-18T09:50:42.762296Z","iopub.status.idle":"2025-10-18T09:51:12.756446Z","shell.execute_reply.started":"2025-10-18T09:50:42.762271Z","shell.execute_reply":"2025-10-18T09:51:12.755818Z"}},"outputs":[{"name":"stderr","text":"2025-10-18 09:50:51.939693: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760781052.127906      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760781052.180198      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fa9367ccbe34b2db2e4f6b2ac7f65c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"706e5839798442d6a1648904d44775b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e08de8e03a54698949cb7c9d727d213"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"802d3d8e710e4970ac88e1df841e9978"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5982f668d6647e0b30a8f271ca1e9cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa9d9425b78f4518a1551e672fb58464"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6f1b2e04ee24762a1374db8049fda6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ddf287dfda1446f91840ce15b17b315"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9b9836ac24740a592ad9d941746d0aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3fbe4c5c87549fe989939cf4791859f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6ef3c52623d40a0a8d85b3daa35526d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"702c068478814b52a95d657c0a1a9430"}},"metadata":{}},{"name":"stdout","text":"                                     id     score  \\\n0  fe25ae1d-4a82-57ad-9bab-b9de4321fd0b  0.024508   \n1  3cbd8443-b3ae-5011-bf59-50f47479a1a7  0.022922   \n2  4184968f-0344-5a58-8c95-1bb6462b95b5  0.022527   \n\n                                               title            brand   price  \\\n0  Karl home Accent Chair Mid-Century Modern Chai...  Karl home Store  149.99   \n1  pranovo Metal Sofa Handle Cable Recliner Chair...          pranovo   13.50   \n2  DBTHTSK Sofa Latch,Bed Replacement Parts,Heavy...          DBTHTSK   12.99   \n\n                                           image_url  \n0  https://m.media-amazon.com/images/I/51+a05Mxh+...  \n1  https://m.media-amazon.com/images/I/3144eTNpeE...  \n2  https://m.media-amazon.com/images/I/41gQlYHLvc...  \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# LOAD GEMMA 2B IT \nimport os, torch\nfrom kaggle_secrets import UserSecretsClient\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")  # ensure you've accepted access on HF\n\nMODEL_ID = \"google/gemma-2b-it\"\nuse_gpu = torch.cuda.is_available()\ntorch_dtype = torch.float16 if use_gpu else torch.float32  # T4 => fp16\n\ntok = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\nif tok.pad_token_id is None:\n    tok.pad_token = tok.eos_token\n\ngen = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    token=HF_TOKEN,\n    device_map=\"auto\",\n    torch_dtype=torch_dtype,   # <-- key change\n)\n\nprint(\"Gemma 2B IT loaded on\", \"GPU (fp16)\" if use_gpu else \"CPU (fp32)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T10:04:36.731605Z","iopub.execute_input":"2025-10-18T10:04:36.732178Z","iopub.status.idle":"2025-10-18T10:04:44.769388Z","shell.execute_reply.started":"2025-10-18T10:04:36.732153Z","shell.execute_reply":"2025-10-18T10:04:44.768576Z"}},"outputs":[{"name":"stderr","text":"`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\nGemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n`config.hidden_activation` if you want to override this behaviour.\nSee https://github.com/huggingface/transformers/pull/29402 for more details.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8224ea79f4314909903401ddf119f7b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b2d4079af0c4346b81bc24ad9cdea71"}},"metadata":{}},{"name":"stdout","text":"Gemma 2B IT loaded on GPU (fp16)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import re, torch\n\ndef gemma_answer(query: str, df_hits: pd.DataFrame, max_new_tokens: int = 240) -> str:\n    titles = [str(t).strip() for t in df_hits.get(\"title\", []).fillna(\"\").tolist() if str(t).strip()]\n    ctx = []\n    for _, r in df_hits.head(5).iterrows():\n        ctx.append(f\"- {r.get('title','N/A')} (Brand: {r.get('brand','N/A')}, Price: {r.get('price','N/A')})\")\n    ctx = \"\\n\".join(ctx) if ctx else \"No context.\"\n\n    prompt = (\n        \"You are a concise, helpful product recommendation assistant.\\n\"\n        \"Rules (follow strictly):\\n\"\n        \"- Never say you cannot answer; if the exact keyword is missing, pick the closest relevant items from the context (chairs/sofas/ottomans/benches/trays) and still answer.\\n\"\n        \"- Do NOT start with 'Sure', 'Okay', or 'Here is/Here’s'. No emojis or meta-chat.\\n\"\n        \"- Write exactly ONE paragraph of 4–6 sentences. Start neutrally (not a brand).\\n\"\n        \"- Mention at least two product titles exactly as in the context. Use only the context.\\n\\n\"\n        f\"Context:\\n{ctx}\\n\\n\"\n        f\"User need:\\n{query}\\n\\n\"\n        \"Write now.\"\n    )\n\n    ins = tok(prompt, return_tensors=\"pt\").to(gen.device)\n    with torch.no_grad():\n        out = gen.generate(\n            input_ids=ins[\"input_ids\"],\n            attention_mask=ins.get(\"attention_mask\"),\n            max_new_tokens=max_new_tokens,\n            min_new_tokens=min(120, max_new_tokens-20),\n            do_sample=False,\n            temperature=0.0,\n            repetition_penalty=1.05,\n            eos_token_id=tok.eos_token_id,\n            pad_token_id=tok.eos_token_id,\n        )\n\n    new_tokens = out[0, ins[\"input_ids\"].shape[1]:]\n    txt = tok.decode(new_tokens, skip_special_tokens=True).strip()\n\n    # sanitize openers + meta\n    txt = re.sub(r\"^(sure,?\\s*|okay,?\\s*|here'?s\\s+.*?:\\s*)\", \"\", txt, flags=re.I).strip()\n    for b in [r\"i cannot answer\", r\"i can't\", r\"unable\", r\"not mention\", r\"no context\",\n              r\"i'm here to assist\", r\"would you like\", r\"let me know\", r\"please note\",\n              r\"i hope this helps\", r\"[😊😁🙂😉👍]\"]:\n        txt = re.sub(b, \"\", txt, flags=re.I)\n    txt = re.sub(r\"\\s+\", \" \", txt).strip()\n\n    # ensure ≥2 titles mentioned\n    needed = []\n    for t in titles[:3]:\n        if t and t not in txt:\n            needed.append(t)\n        if len(needed) >= 2:\n            break\n    if needed:\n        txt += \" In particular, consider \" + \" and \".join(f'\\\"{n}\\\"' for n in needed[:2]) + \".\"\n\n    # clamp to 4–6 sentences\n    sents = [s.strip() for s in re.split(r\"(?<=[.!?])\\s+\", txt) if s.strip()]\n    while len(sents) < 4:\n        sents.append(\"These options balance comfort, value, and everyday usability at home.\")\n    txt = \" \".join(sents[:6])\n\n    # refusal guard: if still refused or <2 titles, build deterministic fallback\n    if re.search(r\"(cannot|can't|unable|no context|not mention)\", txt, re.I) or sum(1 for t in titles if t in txt) < 2:\n        picks = titles[:3]\n        if len(picks) >= 2:\n            base = (\n                \"These options offer practical seating and storage for living spaces. \"\n                f\"\\\"{picks[0]}\\\" and \\\"{picks[1]}\\\" stand out for their everyday comfort and value\"\n                + (f\", while \\\"{picks[2]}\\\" adds a versatile accent.\" if len(picks) > 2 else \".\")\n            )\n        elif len(picks) == 1:\n            base = f\"\\\"{picks[0]}\\\" is a practical choice for compact living spaces with solid everyday value.\"\n        else:\n            base = \"These options balance comfort, value, and everyday usability at home.\"\n        txt = base\n\n    return txt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T10:37:20.628585Z","iopub.execute_input":"2025-10-18T10:37:20.629215Z","iopub.status.idle":"2025-10-18T10:37:20.640000Z","shell.execute_reply.started":"2025-10-18T10:37:20.629190Z","shell.execute_reply":"2025-10-18T10:37:20.639228Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# filter_hits \nimport pandas as pd, re\n\nBAD = (\"lever\",\"latch\",\"cable\",\"release\",\"hardware\",\"bracket\",\"replacement\",\"webbing\",\"band\",\"repair\",\"modification\")\nGOOD = (\"sofa\",\"chair\",\"ottoman\",\"bench\",\"couch\",\"table\",\"tray\",\"armchair\",\"stool\")\n\ndef filter_hits(df: pd.DataFrame) -> pd.DataFrame:\n    if df.empty: \n        return df\n    t = df[\"title\"].fillna(\"\").str.lower()\n    keep = t.apply(lambda x: any(g in x for g in GOOD) and not any(b in x for b in BAD))\n    df2 = df[keep].copy()\n    if len(df2) >= 2:\n        return df2.head(5)\n    util = t.str.contains(r\"(tray|table|ottoman|stool)\", regex=True) & ~t.apply(lambda x: any(b in x for b in BAD))\n    extra = df[util].copy()\n    out = pd.concat([df2, extra]).drop_duplicates(subset=[\"id\"])\n    return out.head(5) if len(out) else df.head(5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T10:37:21.240173Z","iopub.execute_input":"2025-10-18T10:37:21.240741Z","iopub.status.idle":"2025-10-18T10:37:21.246904Z","shell.execute_reply.started":"2025-10-18T10:37:21.240719Z","shell.execute_reply":"2025-10-18T10:37:21.245907Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"#RAG\nimport json\n\ndef _expand_query(q: str) -> str:\n    ql = q.lower()\n    if \"sofa\" in ql:\n        return \"sofa couch chair ottoman bench living room seating\"\n    return q\n\ndef rag(query: str, top_k: int = 8) -> dict:\n    raw = search(_expand_query(query), top_k=top_k)\n    used = filter_hits(raw) if 'filter_hits' in globals() else raw\n    text = gemma_answer(query, used)\n    return {\"recommendations\": used.to_dict(orient=\"records\"), \"generated_text\": text}\n\nsample = rag(\"sofa\", top_k=8)\nprint(sample[\"generated_text\"][:500])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T10:37:22.017432Z","iopub.execute_input":"2025-10-18T10:37:22.017991Z","iopub.status.idle":"2025-10-18T10:37:27.056905Z","shell.execute_reply.started":"2025-10-18T10:37:22.017965Z","shell.execute_reply":"2025-10-18T10:37:27.056139Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e82ca5e3e864f9d849883df4ee11260"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"These options offer practical seating and storage for living spaces. \"Karl home Accent Chair Mid-Century Modern Chair with Pillow Upholstered Lounge Arm Chair with Solid Wood Frame & Soft Cushion for Living Room, Bedroom, Belcony, Beige\" and \"Nalupatio Storage Ottoman, Bedroom End Bench，Upholstered Fabric Storage Ottoman with Safety Hinge, Entryway Padded Footstool, Ottoman Bench for Living Room & Bedroom(Light Green)\" stand out for their everyday comfort and value, while \"Phantoscope Storage Ot\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"\nout_path = \"/kaggle/working/sample_rag_response.json\"\nwith open(out_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(sample, f, ensure_ascii=False, indent=2)\nprint(\"Saved:\", out_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T10:37:34.358015Z","iopub.execute_input":"2025-10-18T10:37:34.358518Z","iopub.status.idle":"2025-10-18T10:37:34.363495Z","shell.execute_reply.started":"2025-10-18T10:37:34.358497Z","shell.execute_reply":"2025-10-18T10:37:34.362906Z"}},"outputs":[{"name":"stdout","text":"Saved: /kaggle/working/sample_rag_response.json\n","output_type":"stream"}],"execution_count":41}]}